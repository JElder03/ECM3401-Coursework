{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "from AdjustedRandomForest import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# CONTROL - NO MISLABELLING\\nscores_my = []\\nscores_std = []\\n\\nfor _ in range(10):\\n    X, y = make_classification(n_samples=3000, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=2,  random_state = 1, flip_y=0.0000001)\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 1)\\n    \\n    rf, corrected_y = train(RandomForestClassifier, X_train, y_train, np.unique(y), 5)\\n    y_test_pred = rf.predict(X_test)\\n    scores_my.append(metrics.accuracy_score(y_test, y_test_pred))\\n\\n    rf = RandomForestClassifier(n_estimators=5, criterion=\\'entropy\\')\\n    rf.fit(X_train, y_train)\\n    y_test_pred = rf.predict(X_test)\\n    scores_std.append(metrics.accuracy_score(y_test, y_test_pred))\\n\\nprint(f\"Test\\nIndividual Accuracies: {scores_my}\\nAverage Accuracy: {np.mean(scores_my)}\\n\")\\nprint(f\"Control\\nIndividual Accuracies: {scores_std}\\nAverage Accuracy: {np.mean(scores_std)}\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# CONTROL - NO MISLABELLING\n",
    "scores_my = []\n",
    "scores_std = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = make_classification(n_samples=3000, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=2,  random_state = 1, flip_y=0.0000001)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 1)\n",
    "    \n",
    "    rf, corrected_y = train(RandomForestClassifier, X_train, y_train, np.unique(y), 5)\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    scores_my.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=5, criterion='entropy')\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    scores_std.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Test\\nIndividual Accuracies: {scores_my}\\nAverage Accuracy: {np.mean(scores_my)}\\n\")\n",
    "print(f\"Control\\nIndividual Accuracies: {scores_std}\\nAverage Accuracy: {np.mean(scores_std)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206\n",
      "(4020, 2)\n",
      "(196, 2)\n",
      "(168, 2)\n",
      "(856, 2)\n",
      "(766, 2)\n",
      "501\n",
      "1206\n",
      "(4020, 2)\n",
      "(80, 2)\n",
      "(62, 2)\n",
      "(22, 2)\n",
      "(52, 2)\n",
      "495\n",
      "1206\n",
      "(4020, 2)\n",
      "(390, 2)\n",
      "(292, 2)\n",
      "(722, 2)\n",
      "(700, 2)\n",
      "521\n",
      "1206\n",
      "(4020, 2)\n",
      "(78, 2)\n",
      "(340, 2)\n",
      "(342, 2)\n",
      "(342, 2)\n",
      "478\n",
      "1206\n",
      "(4020, 2)\n",
      "(436, 2)\n",
      "(622, 2)\n",
      "(516, 2)\n",
      "(606, 2)\n",
      "480\n",
      "1206\n",
      "(4020, 2)\n",
      "(68, 2)\n",
      "(22, 2)\n",
      "(22, 2)\n",
      "(130, 2)\n",
      "497\n",
      "1206\n",
      "(4020, 2)\n",
      "(78, 2)\n",
      "(166, 2)\n",
      "(166, 2)\n",
      "(184, 2)\n",
      "490\n",
      "1206\n",
      "(4020, 2)\n",
      "(38, 2)\n",
      "(30, 2)\n",
      "(52, 2)\n",
      "(44, 2)\n",
      "505\n",
      "1206\n",
      "(4020, 2)\n",
      "(528, 2)\n",
      "(206, 2)\n",
      "(1576, 2)\n",
      "(1356, 2)\n",
      "490\n",
      "1206\n",
      "(4020, 2)\n",
      "(20, 2)\n",
      "(16, 2)\n",
      "(930, 2)\n",
      "(926, 2)\n",
      "490\n",
      "Test\n",
      "Individual Accuracies: [0.5141414141414141, 0.5070707070707071, 0.05454545454545454, 0.5, 0.5060606060606061, 0.5040404040404041, 0.5141414141414141, 0.49696969696969695, 0.5898989898989899, 0.5181818181818182]\n",
      "Average Accuracy: 0.47050505050505054\n",
      "\n",
      "Control\n",
      "Individual Accuracies: [0.5434343434343434, 0.5070707070707071, 0.4696969696969697, 0.5393939393939394, 0.48383838383838385, 0.5262626262626262, 0.503030303030303, 0.509090909090909, 0.5222222222222223, 0.503030303030303]\n",
      "Average Accuracy: 0.5107070707070707\n"
     ]
    }
   ],
   "source": [
    "# MISLABELLING\n",
    "scores_my = []\n",
    "scores_std = []\n",
    "N_CLASSES = 2\n",
    "MISLABELLING =0.4\n",
    "\n",
    "for _ in range(10):\n",
    "    X, y = make_classification(n_samples=3000, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=N_CLASSES, flip_y=0.00000001)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    \n",
    "    y_mislabelled = np.copy(y_train)\n",
    "\n",
    "    for i in range(int(len(y_mislabelled) * MISLABELLING)):\n",
    "        y_mislabelled[i] = (y_mislabelled[i] + random.randint(1, N_CLASSES - 1)) % N_CLASSES\n",
    "\n",
    "    print(sum(i==j for i, j in zip(y_mislabelled, y_train)))\n",
    "    np.random.shuffle(y_mislabelled)\n",
    "\n",
    "    rf, corrected_y = train(RandomForestClassifier, X_train, y_mislabelled, np.unique(y), 5)\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    print(sum(i==j for i, j in zip(y_test_pred, y_train)))\n",
    "    scores_my.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=5, criterion='entropy')\n",
    "    rf.fit(X_train, y_mislabelled)\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    scores_std.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Test\\nIndividual Accuracies: {scores_my}\\nAverage Accuracy: {np.mean(scores_my)}\\n\")\n",
    "print(f\"Control\\nIndividual Accuracies: {scores_std}\\nAverage Accuracy: {np.mean(scores_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {1: [0.59469697, 0.9525066, 0.9525066, 0.98019802, 0.9525066, 0.9525066, 0.59469697, 0.9525066, 0.9525066, 0.9525066, 0.59469697, 0.9525066, 0.9525066, 0.9525066, 0.9525066, 0.59469697, 0.59469697, 0.9525066, 0.59469697, 0.9525066, 0.59469697, 0.9525066, 0.9525066, 0.9525066, 0.98019802, 0.98019802, 0.9525066, 0.9525066, 0.9525066, 0.9525066, 0.59469697, 0.59469697, 0.59469697, 0.98019802, 0.9525066], 0: [0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.9125, 0.40530303, 0.9125, 0.9125, 0.40530303, 0.40530303, 0.40530303, 0.9125, 0.40530303, 0.40530303, 0.9125, 0.9125]})\n",
      "{1: 0.8534400108571428, 0: 0.8364204545}\n",
      "{1: 0.16386823576274817, 0: 0.18110554302143972}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m selection \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[y[i]] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (means[y[i]] \u001b[38;5;241m+\u001b[39m K \u001b[38;5;241m*\u001b[39m standard_deviations[y[i]]) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(e[i][y[i]], means[y[i]] \u001b[38;5;241m+\u001b[39m K \u001b[38;5;241m*\u001b[39m standard_deviations[y[i]], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m):\n\u001b[0;32m    101\u001b[0m         selection\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "e = [[0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.01980198, 0.98019802] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.01980198, 0.98019802] ,\n",
    " [0.01980198, 0.98019802] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.0474934 , 0.9525066 ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.9125    , 0.0875    ] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.40530303, 0.59469697] ,\n",
    " [0.01980198, 0.98019802] ,\n",
    " [0.0474934 , 0.9525066 ]]\n",
    "\n",
    "y = [1,1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0 ,0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1 ,0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1]\n",
    "K = 1\n",
    "# Dictionary to store lists of values corresponding to each unique label\n",
    "grouped_values = defaultdict(list)\n",
    "\n",
    "# Group values by their corresponding label\n",
    "for label, probability in zip(y, e):\n",
    "    grouped_values[label].append(probability[label])\n",
    "\n",
    "print(grouped_values)\n",
    "\n",
    "# Compute mean for each label\n",
    "means = {label: np.mean(probabilities) for label, probabilities in grouped_values.items()}\n",
    "standard_deviations = {label: np.std(probabilities) for label, probabilities in grouped_values.items()}\n",
    "\n",
    "print(means)\n",
    "print(standard_deviations)\n",
    "\n",
    "selection = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if e[i][y[i]] >= (means[y[i]] + K * standard_deviations[y[i]]) or np.isclose(e[i][y[i]], means[y[i]] + K * standard_deviations[y[i]], atol=1e-8):\n",
    "        selection.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
